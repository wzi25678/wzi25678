奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，
以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SVD的。


ShowMeAI知识社区AI知识路径AI实战大全名校AI知识库求职面试宝典大厂技术方案行业名企应用资料大全
斯坦福NLP课程 | 第1讲 - NLP介绍与词向量初步
韩信子,路遥,奇异果2022-04-2846092AINLP自然语言处理梯度下降反向传播word2vec
ShowMeAI用知识加速每一次技术成长

作者：韩信子@ShowMeAI，路遥@ShowMeAI，奇异果@ShowMeAI
教程地址：https://www.showmeai.tech/tutorials/36
本文地址：https://www.showmeai.tech/article-detail/231
声明：版权所有，转载请联系平台与作者并注明出处

收藏ShowMeAI查看更多精彩内容
NLP介绍与词向量初步
ShowMeAI为斯坦福CS224n《自然语言处理与深度学习(Natural Language Processing with Deep Learning)》课程的全部课件，做了中文翻译和注释，并制作成了GIF动图！

词向量、SVD分解与Word2vec
本讲内容的深度总结教程可以在这里 查看。视频和课件等资料的获取方式见文末。

引言
NLP介绍与词向量初步

CS224n是顶级院校斯坦福出品的深度学习与自然语言处理方向专业课程。核心内容覆盖RNN、LSTM、CNN、transformer、bert、问答、摘要、文本生成、语言模型、阅读理解等前沿内容。

ShowMeAI将从本节开始，依托cs224n课程为主框架，逐篇为大家梳理NLP的核心重点知识原理。

本篇内容覆盖：
Introduction and Word Vectors

第1课直接切入语言和词向量，讲解了自然语言处理的基本概念，文本表征的方法和演进，包括word2vec等核心方法，词向量的应用等。

自然语言与文字
word2vec介绍
word2vec目标函数与梯度
算法优化基础
word2vec构建的词向量模式
1. 自然语言与词汇含义
1.1 人类的语言与词汇含义
咱们先来看看人类的高级语言。

人类的语言与词汇含义

人类之所以比类人猿更“聪明”，是因为我们有语言，因此是一个人机网络，其中人类语言作为网络语言。人类语言具有信息功能和社会功能。

据估计，人类语言只有大约5000年的短暂历史。语言和写作是让人类变得强大的原因之一。它使知识能够在空间上传送到世界各地，并在时间上传送。

但是，相较于如今的互联网的传播速度而言，人类语言是一种缓慢的语言。然而，只需人类语言形式的几百位信息，就可以构建整个视觉场景。这就是自然语言如此迷人的原因。

1.2 我们如何表达一个词的意思？
我们如何表达一个词的意思？

我们如何表达一个词的含义呢？有如下一些方式：

用一个词、词组等表示的概念
一个人想用语言、符号等来表达的想法
表达在作品、艺术等方面的思想


理解意义的最普遍的语言方式(linguistic way)：语言符号与语言意义（想法、事情）的相互对应

denotational semantics：语义
公式
1.3 如何在计算机里表达词的意义
要使用计算机处理文本词汇，一种处理方式是WordNet：即构建一个包含同义词集和上位词(“is a”关系)的列表的辞典。英文当中确实有这样一个 wordnet，我们在安装完NLTK工具库和下载数据包后可以使用，对应的 python 代码如下：

from nltk.corpus import wordnet as wn
poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}
for synset in wn.synsets("good"):
        print("{}: {}".format(poses[synset.pos()], ", ".join([l.name() for l in synset.lemmas()])))
from nltk.corpus import wordnet as wn
panda = wn.synset("panda.n.01")
hyper = lambda s: s.hypernyms()
list(panda.closure(hyper))
结果如下图所示：

如何在计算机里表达词的意义

1.4 WordNet的问题
WordNet的问题

WordNet大家可以视作1个专家经验总结出来的词汇表，但它存在一些问题：

1
1 忽略了词汇的细微差别

例如“proficient”被列为“good”的同义词。这只在某些上下文中是正确的。
2
2 缺少单词的新含义

难以持续更新！
例如：wicked、badass、nifty、wizard、genius、ninja、bombast
3
3 因为是小部分专家构建的，有一定的主观性

4
4 构建与调整都需要很多的人力成本

5
5 无法定量计算出单词相似度

1.5 文本(词汇)的离散表征
文本(词汇)的离散表征

在传统的自然语言处理中，我们会对文本做离散表征，把词语看作离散的符号：例如hotel、conference、motel等。


一种文本的离散表示形式是把单词表征为独热向量(one-hot vectors)的形式

独热向量：只有一个1，其余均为0的稀疏向量


在独热向量表示中，向量维度 = 词汇量(如500,000)，以下为一些独热向量编码过后的单词向量示例：

公式

公式
1.6 离散表征的问题
离散表征的问题

在上述的独热向量离散表征里，所有词向量是正交的，这是一个很大的问题。对于独热向量，没有关于相似性概念，并且向量维度过大。



对于上述问题有一些解决思路：

① 使用类似WordNet的工具中的列表，获得相似度，但会因不够完整而失败
② 通过大量数据学习词向量本身相似性，获得更精确的稠密词向量编码
1.7 基于上下文的词汇表征
近年来在深度学习中比较有效的方式是基于上下文的词汇表征。它的核心想法是：一个单词的意思是由经常出现在它附近的单词给出的 “You shall know a word by the company it keeps” (J. R. Firth 1957: 11)。

这是现代统计NLP最成功的理念之一，总体思路有点物以类聚，人以群分的感觉。

当一个单词 公式 出现在文本中时，它的上下文是出现在其附近的一组单词(在一个固定大小的窗口中)
基于海量数据，使用 公式 的许多上下文来构建 公式 的表示
如图所示，banking的含义可以根据上下文的内容表征。

基于上下文的词汇表征

2.Word2vec介绍
2.1 词向量表示
词向量表示

下面我们要介绍词向量的构建方法与思想，我们希望为每个单词构建一个稠密表示的向量，使其与出现在相似上下文中的单词向量相似。

词向量(word vectors)有时被称为词嵌入(word embeddings)或词表示(word representations)。
稠密词向量是分布式表示(distributed representation)。
2.2 Word2vec原理介绍
Word2vec原理介绍

Word2vec (Mikolov et al. 2013)是一个学习词向量表征的框架。



核心思路如下：

基于海量文本语料库构建
词汇表中的每个单词都由一个向量表示（学习完成后会固定）
对应语料库文本中的每个位置 公式，有一个中心词 公式 和一些上下文(“外部”)单词 公式
使用 公式 和 公式 的词向量来计算概率 公式，即给定中心词推断上下文词汇的概率（反之亦然）
不断调整词向量来最大化这个概率


下图为窗口大小 公式 时的 公式，它的中心词为 公式

Word2vec原理介绍



下图为窗口大小 公式 时的 公式，它的中心词为 公式

Word2vec原理介绍

3.Word2vec 目标函数
3.1 Word2vec目标函数
我们来用数学表示的方式，对word2vec方法做一个定义和讲解。

3.1.1 似然函数
对于每个位置 公式，在大小为 公式 的固定窗口内预测上下文单词，给定中心词 公式，似然函数可以表示为：

公式
上述公式中，公式 为模型包含的所有待优化权重变量

3.1.2 目标函数
Word2vec目标函数

对应上述似然函数的目标函数 公式 可以取作(平均)负对数似然：

公式
注意：

目标函数 公式 有时也被称为“代价函数”或“损失函数”
最小化目标函数 公式 最大化似然函数（预测概率/精度），两者等价


补充解读

上述目标函数中的log形式是方便将连乘转化为求和，负号是希望将极大化似然率转化为极小化损失函数的等价问题
在连乘之前使用log转化为求和非常有效，特别是做优化时
公式


得到目标函数后，我们希望最小化目标函数，那我们如何计算 公式 ？

Word2vec目标函数

对于每个词 公式 都会用两个向量：

当 公式 是中心词时，我们标记词向量为 公式
当 公式 是上下文词时，我们标记词向量为 公式


则对于一个中心词 公式 和一个上下文词 公式，我们有如下概率计算方式：公式

Word2vec目标函数

对于上述公式，ShowMeAI 做一点补充解读：

公式中，向量 公式 和向量 公式 进行点乘
向量之间越相似，点乘结果越大，从而归一化后得到的概率值也越大
模型的训练正是为了使得具有相似上下文的单词，具有相似的向量
点积是计算相似性的一种简单方法，在注意力机制中常使用点积计算 Score，参见 ShowMeAI 文章 深度学习教程 | Seq2Seq序列模型和注意力机制。
3.2 从向量视角回顾Word2vec
从向量视角回顾Word2vec

下图为计算 公式 的示例，这里把 公式 简写为 公式，例子中的上下文窗口大小2，即“左右2个单词+一个中心词”。

4.Word2vec prediction function
4.1 Word2vec预测函数
回到上面的概率计算，我们来观察一下

Word2vec预测函数

公式
取幂使任何数都为正
点积比较 公式 和 公式 的相似性 公式，点积越大则概率越大
分母：对整个词汇表进行标准化，从而给出概率分布


这里有一个softmax的概率，softmax function 公式 示例：

将任意值 公式 映射到概率分布 公式

公式


其中对于名称中soft和max的解释如下（softmax在深度学习中经常使用到）：

max：因为放大了最大的概率
soft：因为仍然为较小的 公式 赋予了一定概率
4.2 word2vec中的梯度下降训练细节推导
下面是对于word2vec的参数更新迭代，应用梯度下降法的一些推导细节，ShowMeAI写在这里做一点补充。

首先我们随机初始化 公式 和 公式，而后使用梯度下降法进行更新

公式


偏导数可以移进求和中，对应上方公式的最后两行的推导

公式


我们可以对上述结果重新排列如下，第一项是真正的上下文单词，第二项是预测的上下文单词。使用梯度下降法，模型的预测上下文将逐步接近真正的上下文。

公式


再对 公式 进行偏微分计算，注意这里的 公式 是 公式 的简写，故可知

公式 公式


可以理解，当 公式，即通过中心词 公式 我们可以正确预测上下文词 公式，此时我们不需要调整 公式，反之，则相应调整 公式 。关于此处的微积分知识，可以查阅ShowMeAI的教程图解AI数学基础文章图解AI数学基础 | 微积分与最优化。

word2vec中的梯度下降训练细节推导

训练模型的过程，实际上是我们在调整参数最小化损失函数。
如下是一个包含2个参数的凸函数，我们绘制了目标函数的等高线。
4.3 训练模型：计算所有向量梯度
训练模型：计算所有向量梯度

公式 代表所有模型参数，写在一个长的参数向量里。

在我们的场景汇总是 公式 维向量空间的 公式 个词汇。

5.视频教程
可以点击 B站 查看视频的【双语字幕】版本


6.参考资料
本讲带学的在线阅翻页本
《斯坦福CS224n深度学习与自然语言处理》课程学习指南
《斯坦福CS224n深度学习与自然语言处理》课程大作业解析
【双语字幕视频】斯坦福CS224n | 深度学习与自然语言处理(2019·全20讲)
Stanford官网 | CS224n: Natural Language Processing with Deep Learning
ShowMeAI系列教程推荐
大厂技术实现 | 推荐与广告计算解决方案
大厂技术实现 | 计算机视觉解决方案
大厂技术实现 | 自然语言处理行业解决方案
图解Python编程：从入门到精通系列教程
图解数据分析：从入门到精通系列教程
图解AI数学基础：从入门到精通系列教程
图解大数据技术：从入门到精通系列教程
图解机器学习算法：从入门到精通系列教程
机器学习实战：手把手教你玩转机器学习系列
深度学习教程 | 吴恩达专项课程 · 全套笔记解读
自然语言处理教程 | 斯坦福CS224n课程 · 课程带学与全套笔记解读
NLP系列教程文章
NLP教程(1)- 词向量、SVD分解与Word2vec
NLP教程(2)- GloVe及词向量的训练与评估
NLP教程(3)- 神经网络与反向传播
NLP教程(4)- 句法分析与依存解析
NLP教程(5)- 语言模型、RNN、GRU与LSTM
NLP教程(6)- 神经机器翻译、seq2seq与注意力机制
NLP教程(7)- 问答系统
NLP教程(8)- NLP中的卷积神经网络
NLP教程(9)- 句法分析与树形递归神经网络
斯坦福 CS224n 课程带学详解
斯坦福NLP课程 | 第1讲 - NLP介绍与词向量初步
斯坦福NLP课程 | 第2讲 - 词向量进阶
斯坦福NLP课程 | 第3讲 - 神经网络知识回顾
斯坦福NLP课程 | 第4讲 - 神经网络反向传播与计算图
斯坦福NLP课程 | 第5讲 - 句法分析与依存解析
斯坦福NLP课程 | 第6讲 - 循环神经网络与语言模型
斯坦福NLP课程 | 第7讲 - 梯度消失问题与RNN变种
斯坦福NLP课程 | 第8讲 - 机器翻译、seq2seq与注意力机制
斯坦福NLP课程 | 第9讲 - cs224n课程大项目实用技巧与经验
斯坦福NLP课程 | 第10讲 - NLP中的问答系统
斯坦福NLP课程 | 第11讲 - NLP中的卷积神经网络
斯坦福NLP课程 | 第12讲 - 子词模型
斯坦福NLP课程 | 第13讲 - 基于上下文的表征与NLP预训练模型
斯坦福NLP课程 | 第14讲 - Transformers自注意力与生成模型
斯坦福NLP课程 | 第15讲 - NLP文本生成任务
斯坦福NLP课程 | 第16讲 - 指代消解问题与神经网络方法
斯坦福NLP课程 | 第17讲 - 多任务学习(以问答系统为例)
斯坦福NLP课程 | 第18讲 - 句法分析与树形递归神经网络
斯坦福NLP课程 | 第19讲 - AI安全偏见与公平
斯坦福NLP课程 | 第20讲 - NLP与深度学习的未来


斯坦福NLP课程 | 第1讲 - NLP介绍与词向量初步
NLP教程(1) - 词向量、SVD分解与Word2Vec

NLP教程(2) - GloVe及词向量的训练与评估

全部评论
2022-07-14 14:14
请问这门课你们是怎么学的？ 我发现老师的视频讲课讲得一般般，像讲故事一样

2022-07-10 16:29
请问有pdf版吗

发表评论
别默默的看了,快来点评一下吧...
您的评论会经过筛选后显示发表
公众号

扫码关注公众号

和我们一起成长

圈子
AI 海量资料仓库博客
一键运行所有代码Github
AI学习视频宝藏库Bilibili
收藏！历史干货合辑知乎
最前沿的技术动态微博
ShowMeAI掘金








关于
ShowMeAI——人工智能领域的资料库和学习社区，覆盖Python、数据科学、机器学习、深度学习、自然语言处理、计算机视觉等方向。我们为学习、求职、项目落地、业务探索等场景，提供了结构化路径和全套资料库。构建AI解决方案，用知识加速每一次技术成长！ShowMeAI，只做精品。

热门标签
机器学习计算机视觉回归树神经网络速查表sparkdeep learning随机森林模型选择pipelineAIRNN自然语言处理python吴恩达CNNLightGBMCV实战梯度下降GBDT决策树人工智能pandascs230工具教程大数据PCANLP图像识别XGBoost数据挖掘编程语言数学推荐系统深度学习数据分析注意力机制算法数据探索学习路径可视化cs224n超参数调优cs231n反向传播sklearn调参word2vec课程
关注我们

© 2021-2023 ShowMeAI. All Rights Reserved. 京ICP备2021026670号-1 京公网安备11010802038987号
×
拖拽到此处
图片将完成下载
